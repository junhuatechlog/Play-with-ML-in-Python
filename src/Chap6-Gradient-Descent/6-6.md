# 随机梯度下降法 Stochastic Gradient Descent

每次只取出第i个样本进行计算，得到的值决定了下一次的搜索方向，有可能往梯度增大的方向前进，但是总的来说会逼近最小值。尤其是当样本数m值很大时，我们愿意用精度换取时间。

为了避免我们已经到loss function的最小值附近，但是因为学习率过大，导致又跳出了最小值附近，我们希望学习率是逐渐递减的。 