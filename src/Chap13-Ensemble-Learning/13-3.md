# Bagging and Pasting

如何保证每个子模型创建的差异性：
每个子模型只看样本数据的一部分。

每个子模型的预测准确率低 - 并不需要太高的准确率。 

如果每个子模型的预测准确率是60%， 500个子模型，总的准确率是：
$$\sum_{i=251}^{500} C_{500}^i \cdot 0.6^i \cdot 0.4^{500-i} = 99.999\% $$

**在算法比赛中，集成学习的使用率是最高的**。

每个子模型只看样本数据的一部分：
- **放回取样 - Bagging** 
- 不放回取样 - Pasting - 每个模型看到的样本是不重复的，不一样的

Bagging更常用, 如果不放回取样，则训练子模型数量太少。 - 统计学中叫 bootstrap
Bagging没有特别依赖于随机，pasting中，分子样本会强烈影响集成学习所得的结果。

**DecisionTreeClassifier 产生差异较大的模型，更加随机，集成学习要求子模型有区别，所以， 一般都用决策树做集成学习**。


