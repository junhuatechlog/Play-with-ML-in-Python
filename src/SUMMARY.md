# Summary

* [Introduction](README.md)
* [Chapter1 基础]()
   * [Jupyter Notebook](Chap1-Basics/Jupyter-Notebook-tips.md)
   * [Matplotlib](Chap1-Basics/Matplotlib.md)
   * [mdbook](Chap1-Basics/mdBookTips.md)
   * [Numpy](Chap1-Basics/numpy.md)
   * [Python Class](Chap1-Basics/python-class.md)
* [Chapter4 最基础的分类算法-k近邻算法kNN]()
* [Chapter5 线性回归法]()
* [Chapter6 梯度下降法]()
   * [梯度下降法](Chap6-Gradient-Descent/6-1.md)
   * [模拟实现梯度下降法](Chap6-Gradient-Descent/6-2.md)
   * [线性回归中的梯度下降法](Chap6-Gradient-Descent/6-3.md)
   * [实现线性回归中的梯度下降法](Chap6-Gradient-Descent/6-4.md)
   * [梯度下降法的向量化和数据标准化](Chap6-Gradient-Descent/6-5.md)
   * [随机梯度下降法](Chap6-Gradient-Descent/6-6.md)
   * [scikit-learn 中的随机梯度下降法](Chap6-Gradient-Descent/6-7.md)
   * [如何判断梯度下降法中梯度值是正确的](Chap6-Gradient-Descent/6-8.md)
   * [小批量梯度下降法](Chap6-Gradient-Descent/6-9.md)
* [Chapter7 PCA和梯度上升法]()
   * [PCA](Chap7-PCA/7-1.md)
   * [使用梯度上升法求解PCA问题](Chap7-PCA/7-2.md)
   * [求数据主成分PCA](Chap7-PCA/7-3.md)
   * [求数据的前n个主成分](Chap7-PCA/7-4.md)
   * [高维数据向低维数据映射](Chap7-PCA/7-5.md)
   * [scikit-learn中的PCA](Chap7-PCA/7-6.md)
   * [MNIST 数据集](Chap7-PCA/7-7.md)
* [Chapter8 多项式回归和模型泛化]()  
   * [scikit-learn中的多项式回归和Pipeline](Chap8-PolynomialRegression/8-2.md)
   * [过拟合和欠拟合](Chap8-PolynomialRegression/8-3.md)
   * [训练数据集和测试数据集](Chap8-PolynomialRegression/8-4.md)
   * [学习曲线](Chap8-PolynomialRegression/8-5.md)
   * [验证数据集与交叉验证](Chap8-PolynomialRegression/8-6.md)
   * [偏差方差权衡](Chap8-PolynomialRegression/8-7.md)
   * [岭回归](Chap8-PolynomialRegression/8-8.md)
   * [LASSO回归](Chap8-PolynomialRegression/8-9.md)
   * [弹性网络](Chap8-PolynomialRegression/8-10.md)
* [Chapter9 逻辑回归]()  
   * [逻辑回归](Chap9-Logistic-Regression/9-1.md)
   * [逻辑回归的损失函数](Chap9-Logistic-Regression/9-2.md)
   * [逻辑回归损失函数的梯度](Chap9-Logistic-Regression/9-3.md)
   * [实现逻辑回归算法](Chap9-Logistic-Regression/9-4.md)
   * [决策边界 - Decision Boundary](Chap9-Logistic-Regression/9-5.md)
   * [逻辑回归中使用多项式特征](Chap9-Logistic-Regression/9-6.md)
   * [逻辑回归中使用正则化](Chap9-Logistic-Regression/9-7.md)
   * [OvR 与 OvO - 逻辑回归解决多分类问题](Chap9-Logistic-Regression/9-8.md)
* [Chapter10 评价分类结果]()  
   * [准确度陷阱和混淆矩阵](Chap10-Classification-Performance-Measures/10-1.md)
   * [精准率和召回率的平衡](Chap10-Classification-Performance-Measures/10-2.md)
   * [精准率-召回率曲线](Chap10-Classification-Performance-Measures/10-3.md)
   * [ROC曲线 ](Chap10-Classification-Performance-Measures/10-4.md)
   * [多分类问题中的混淆矩阵 ](Chap10-Classification-Performance-Measures/10-5.md)
* [Chapter11 SVM]()  
   * [SVM](Chap11-SVM/11-1.md)
   * [SVM 背后的最优化问题](Chap11-SVM/11-2.md)
   * [Soft Margin SVM 和SVM 正则化](Chap11-SVM/10-3.md)
   * [scikit-learn中的SVM](Chap11-SVM/10-4.md)
   * [SVM中使用多项式特征和核函数](Chap11-SVM/10-5.md)
   * [核函数 - 多项式核为例](Chap11-SVM/11-6.md)
   * [高斯核函数](Chap11-SVM/11-7.md)
   * [BF核函数中的gamma](Chap11-SVM/11-8.md)
   * [使用SVM解决回归问题](Chap11/11-9.md)
* [Chapter12 决策树]()  
   * [决策树](Chap12-DecisionTree/12-1.md)
   * [信息熵](Chap12-DecisionTree/12-2.md)
   * [使用信息熵寻找最优划分](Chap12-DecisionTree/12-3.md)
   * [基尼系数](Chap12-DecisionTree/12-4.md)
   * [CART 与决策树中的超参数](Chap12-DecisionTree/12-5.md)
   * [决策树解决回归问题](Chap12-DecisionTree/12-6.md)
   * [决策树的局限性](Chap12-DecisionTree/12-7.md)
* [Chapter13 Ensemble Learning 集成学习]()  
   * [集成学习](Chap13-Ensemble-Learning/13-1.md)
   * [Soft Voting Classifier](Chap13-Ensemble-Learning/13-2.md)
   * [Bagging and Pasting](Chap13-Ensemble-Learning/13-3.md)
   * [OOB(Out of Bag) 和关于Bagging的更多讨论](Chap13-Ensemble-Learning/13-4.md)
   * [随机森林和Extra-Trees](Chap13-Ensemble-Learning/13-5.md)
   * [Ada Boosting 和Gradient Boosting](Chap13-Ensemble-Learning/13-6.md)
   * [Stacking](Chap13-Ensemble-Learning/13-7.md)